{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nodes for the real data experiments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "327aZS9Hcwse"
      },
      "source": [
        "This notebook contains some simple code for viewing the node IDs from the Microsoft Academic Graph that we used for the real data experiments in our paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6_MjbUTbKww"
      },
      "source": [
        "import pickle\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQDP8dHcbqsc"
      },
      "source": [
        "DATA_DIR = \"data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctYbV-9tbhJs"
      },
      "source": [
        "# Dataset 1.\n",
        "\n",
        "# Each list contains the IDs of the nodes from the Microsoft Academic Graph that\n",
        "# we use in our experiments.\n",
        "train_nodes = pickle.load(open(os.path.join(DATA_DIR, \"dataset_1/train_nodes.pkl\"), \"rb\"))\n",
        "val_nodes = pickle.load(open(os.path.join(DATA_DIR, \"dataset_1/val_nodes.pkl\"), \"rb\"))\n",
        "test_nodes = pickle.load(open(os.path.join(DATA_DIR, \"dataset_1/test_nodes.pkl\"), \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECjJv-yThBOb"
      },
      "source": [
        "class PaperIdAndIndexMap:\n",
        "    \n",
        "    def __init__(self, topo_sorted_nodes):\n",
        "        self.paper_id_to_idx = {}\n",
        "        self.idx_to_paper_id = {}\n",
        "        for idx, paper_id in enumerate(topo_sorted_nodes):\n",
        "            self.paper_id_to_idx[paper_id] = idx\n",
        "            self.idx_to_paper_id[idx] = paper_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-k4vymTIb3lG",
        "outputId": "55f75151-a88e-4213-ddfb-088da859ab4c"
      },
      "source": [
        "print(\"Number of training nodes: %d\" % len(train_nodes))\n",
        "print(\"Number of validation nodes: %d\" % len(val_nodes))\n",
        "print(\"Number of test nodes: %d\" % len(test_nodes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training nodes: 1709405\n",
            "Number of validation nodes: 244200\n",
            "Number of test nodes: 488403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc-2RJIkb7nn"
      },
      "source": [
        "# Dataset 2.\n",
        "\n",
        "# Each list contains the IDs of the nodes from the Microsoft Academic Graph that\n",
        "# we use in our experiments.\n",
        "train_nodes = pickle.load(open(os.path.join(DATA_DIR, \"dataset_2/train_nodes.pkl\"), \"rb\"))\n",
        "val_nodes = pickle.load(open(os.path.join(DATA_DIR, \"dataset_2/val_nodes.pkl\"), \"rb\"))\n",
        "test_nodes = pickle.load(open(os.path.join(DATA_DIR, \"dataset_2/test_nodes.pkl\"), \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIUV2y8Qco4F",
        "outputId": "b55d4e62-e764-43c8-b122-995693806f6f"
      },
      "source": [
        "print(\"Number of training nodes: %d\" % len(train_nodes))\n",
        "print(\"Number of validation nodes: %d\" % len(val_nodes))\n",
        "print(\"Number of test nodes: %d\" % len(test_nodes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training nodes: 930064\n",
            "Number of validation nodes: 132866\n",
            "Number of test nodes: 265734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Yf9tzrtNDob"
      },
      "source": [
        "In our paper, we preprocess the paper text (title and abstract) and 768-dimensional embedding using [SciBERT](https://github.com/allenai/scibert) and [bert-as-a-service](https://github.com/hanxiao/bert-as-service).\n",
        "\n",
        "We provide the preprocessed data with the 768-dimensional embeddings for both\n",
        "the datasets used in our paper. The data is in the form of a dictionary that maps each paper ID to its text embedding and can be loaded using the `pickle` library. The dataset can be found [here](https://drive.google.com/file/d/1cfR6strHk3SoSUHbYv_yY1fXbgWZaP5T/view?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wGBImRMNH0O"
      },
      "source": [
        "# A dictionary where the key is Microsoft Academic Graph node ID representing\n",
        "# an academic paper and the value is a 768-dimensional `np.array` representing\n",
        "# the text embedding that we use in our training pipeline.\n",
        "# `nodes_to_scibert_embedding_dataset_{1,2}.pkl` can be downloaded from the links above.\n",
        "nodes_to_scibert = pickle.load(open(os.path.join(DATA_DIR, \"nodes_to_scibert_embedding_dataset_1.pkl\"), \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}